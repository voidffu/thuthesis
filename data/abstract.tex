% !TeX root = ../thuthesis-example.tex

% 中英文摘要和关键字

\begin{abstract}
  手势是人类交流信息和表达意图的重要方法\cite{guo2021human}。近年来，手势驱动的人机交互技术取得了显著的进展\cite{伍杰2019基于视觉的实时手势识别方法研究, desai2017human,strickland2013using}，使得手势识别（HGR）与手势生成（HGG）成为研究的热点之一。
  手语是听障人士日常交流的重要媒介，其学习与普及在社会交往中具有重要意义。然而，标准手语的推广受限，手语学习难度较高，且优质教学资源匮乏。基于手势的人工智能技术有望为手语辅助教学问题提供新的解决方案。本文基于深度学习技术，研究构建了多模态手势识别与协同手势生成算法；基于此，围绕手语学习过程中的核心挑战，设计实现了一个交互式手语学习助手系统。主要创新贡献如下：

  （1）多模态手势识别算法创新：提出了一种可插拔的多策略解耦与语义集成网络（MDSI），通过“姿势-运动”与“时空-通道”特征解耦，有效降低RGB-D手势识别中的信息冗余，并结合语义滤波与标签平滑机制提升语义区分能力。实验结果表明，该方法在IsoGD和THU-READ数据集上分别超越现有最优方法2.48\%和4.33\%，同时保持轻量化设计，其附加参数量仅占主干网络的6.84\%。
  
  （2）协同手势生成算法创新：提出了 CoordSpeaker，一种新颖的协同字幕赋能的同声手势生成方法，实现了手势运动生成中的节奏同步和语义对齐。通过首次引入手势描述生成模块，有效解决了手势数据缺乏文本标注的问题。结合统一的运动表示与可控潜在扩散模型，实现语义与节奏的精准协同控制。实验表明，该方法可生成高质量的（Jerk $0.179\rightarrow$）、语音同步（BC $0.057\uparrow$）、语义相关（MM-Dist: $6.814$）的协同手势运动，显著领先同类方法。同时，将平均推理时间（AITS）降低至0.842秒，较现有方法加速6倍以上，极大提升了实际应用价值。
  
  （3）交互式手语学习系统构建：基于上述算法，设计并实现了一套集成实时识别、标准动作生成与交互反馈的手语学习系统。系统采用模块化架构，支持手语学习、练习评估和自由练习三种核心交互模式，并提供清晰的界面布局与友好的交互设计。在RGB-D相机上的实验验证表明，系统在12类数据上的识别准确率超过99\%，推理延迟小于0.1秒；用户研究表明，生成的手部动作自然度偏好较同类方法提升4.65\%，充分验证了所提算法的实用价值。
  
  本文提出的手势识别与生成算法不仅显著提升了识别准确率与生成质量，同时也为解决手语教学资源匮乏问题、提升手语学习效率提供了新的技术路径。

  % 关键词用“英文逗号”分隔，输出时会自动处理为正确的分隔符
  \thusetup{
    keywords = {手势识别, 手势生成, 多模态, 手语学习, 交互式系统},
  }
\end{abstract}

\begin{abstract*}
Gestures are a vital means of conveying information and expressing intentions in human communication \cite{guo2021human}. In recent years, gesture-based human-computer interaction technologies have achieved remarkable progress \cite{伍杰2019基于视觉的实时手势识别方法研究, desai2017human,strickland2013using}, making gesture recognition (HGR) and gesture generation (HGG) prominent research topics. 
  Sign language serves as a crucial medium for daily communication among the hearing-impaired population, and its learning and popularization hold significant social importance. However, the promotion of standardized sign language faces limitations due to high learning difficulty and scarcity of quality educational resources. Gesture-based artificial intelligence technology offers promising solutions for sign language assisted teaching. Based on deep learning techniques, this thesis develops multimodal gesture recognition and collaborative gesture generation algorithms. Building upon these, we design and implement an interactive sign language learning assistant system that addresses core challenges in the sign language learning process. The main innovative contributions are as follows:

(1) Innovation in multimodal gesture recognition: We propose a plug-and-play Multi-strategy Decoupling and Semantic Integration Network (MDSI), which effectively reduces information redundancy in RGB-D gesture recognition by introducing "pose-motion" and "spatiotemporal-channel" feature decoupling. Additionally, semantic filtering and label smoothing mechanisms enhance semantic distinction. Experimental results demonstrate that MDSI surpasses the state-of-the-art methods by 2.48\% and 4.33\% on the IsoGD and THU-READ datasets, respectively. Furthermore, the model maintains a lightweight design, with additional parameters accounting for only 6.84\% of the backbone network.

(2) Innovation in collaborative gesture generation: We propose CoordSpeaker, a novel coordinated caption-empowered co-speech gesture generation approach, realizing both rhythmic synchronization and semantic alignment in speaker motion generation. By introducing a gesture caption generation module for the first time, we effectively address the absence of textual annotations in gesture datasets. By leveraging a unified motion representation and a controllable latent diffusion model, our approach achieves precise coordination of semantic consistency and rhythmic alignment. Experimental results indicate that the proposed method generates high-quality gestures that align well with speech content and semantic intent, receiving significantly higher user preference than competing methods. Moreover, an optimized latent diffusion process reduces the average inference time (AITS) to 0.406 seconds, achieving a 6-fold speedup over existing approaches and substantially enhancing practical applicability.

(3) Development of an interactive sign language learning system: Based on the proposed algorithms, we design and implement an integrated system that supports real-time recognition, standard gesture generation, and interactive feedback. The system employs a modular architecture and offers three core interaction modes: sign language learning, practice evaluation, and free practice, with an intuitive interface layout and user-friendly interaction design. Experimental validation with an RGB-D camera shows that the system achieves a recognition accuracy exceeding 99\% for 12 gesture classes, with an inference latency of less than 0.1 seconds. Additionally, the naturalness preference for generated gestures surpasses existing methods by 17.22\%, demonstrating the practical value of the proposed algorithms.

The proposed gesture recognition and generation algorithms not only significantly improve recognition accuracy and generation quality but also provide a novel technical approach to addressing the shortage of sign language educational resources and enhancing learning efficiency.

  % Use comma as separator when inputting
  \thusetup{
    keywords* = {Gesture Recognition, Gesture Generation, Multimodal Learning, Sign Language Learning, Interactive System},
  }
\end{abstract*}
